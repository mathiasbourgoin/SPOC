# Benchmarks TODO

This document tracks the implementation status of the Sarek benchmark suite.

## Infrastructure

- [x] Common library (statistics, timing, system info)
- [x] Backend loader with conditional GPU support
- [x] JSON output format with full system metadata
- [x] CSV conversion tool
- [x] Aggregation tool for multi-machine results
- [x] Web viewer (to_web.ml) for GitHub Pages
- [x] Interactive Chart.js visualization
- [x] PR preview deployment workflow
- [ ] Plotting tools (gnuplot or OCaml-based)
- [ ] LaTeX table generation
- [ ] Unified benchmark runner (bench_runner.exe)
- [ ] CI integration for performance regression tracking

## Core Performance Benchmarks

### Memory Bandwidth
- [x] **Vector Add** - Pure memory bandwidth test
  - ‚úÖ Implemented in bench_vector_add.ml
  - ‚úÖ Element-wise addition: `C[i] = A[i] + B[i]`
  - ‚úÖ Measures memory bandwidth (GB/s)
  - ‚úÖ Default sizes: 1M, 10M, 50M, 100M elements
  - ‚úÖ CPU baseline verification
  
- [ ] **Vector Copy** - Memory transfer baseline
  - Simple copy: `B[i] = A[i]`
  - Baseline for memory operations
  
- [ ] **STREAM Triad** - Industry standard memory benchmark
  - `A[i] = B[i] + C[i] * scalar`
  - Compare against published STREAM results

### Linear Algebra
- [x] **Matrix Multiplication (naive)** - Basic dense linear algebra
  - ‚úÖ Implemented in bench_matrix_mul.ml
  - ‚úÖ CPU baseline verification
  - ‚úÖ Throughput calculation (GFLOPS)
  - ‚úÖ Bug fixed: correct kernel arguments (m, n, k)
  - ‚úÖ Default sizes: 256, 512, 1024, 2048 elements
  
- [x] **Matrix Multiplication (tiled)** - Shared memory optimization
  - ‚úÖ Implemented in bench_matrix_mul_tiled.ml
  - ‚úÖ Uses 16√ó16 tiles with shared memory
  - ‚úÖ Shows optimization impact
  - ‚úÖ Compare naive vs tiled performance
  - ‚úÖ Default sizes: 128, 256, 512, 1024, 2048, 4096
  
- [ ] **Matrix Multiplication (optimized)** - Register blocking
  - Advanced optimizations
  - Multiple tile sizes
  - Compare against cuBLAS/clBLAS
  
- [ ] **Matrix Multiplication (optimized)** - Register blocking
  - Advanced optimizations
  - Multiple tile sizes
  - Compare against cuBLAS/clBLAS

- [ ] **GEMM (SGEMM/DGEMM)** - BLAS-level routines
  - Float32 and Float64 variants
  - Batched matrix multiply
  - Matrix sizes from 128x128 to 8192x8192

### Parallel Reduction
- [x] **Sum Reduction** - Basic parallel reduction
  - ‚úÖ Implemented in bench_reduction.ml
  - ‚úÖ Tree-based reduction with shared memory
  - ‚úÖ Logarithmic reduction pattern (256 -> 128 -> 64 -> ... -> 1)
  - ‚úÖ Default sizes: 1M, 10M, 50M, 100M elements
  - ‚úÖ Verification passing on all sizes
  - ‚úÖ Measures memory bandwidth (GB/s)
  
- [x] **Min/Max Reduction** - Comparison-based reduction
  - ‚úÖ Implemented in bench_reduction_max.ml
  - ‚úÖ Find maximum in array with tree reduction
  - ‚úÖ Shared memory optimization
  - ‚úÖ Default sizes: 1M, 10M, 50M, 100M elements
  
- [x] **Dot Product** - Combined multiply-reduce
  - ‚úÖ Implemented in bench_dot_product.ml
  - ‚úÖ `sum(A[i] * B[i])`
  - ‚úÖ Common in scientific computing
  - ‚úÖ Default sizes: 1M, 10M, 50M, 100M elements

### Data Movement
- [x] **Transpose (Naive)** - Memory access pattern benchmark
  - ‚úÖ Implemented in bench_transpose.ml
  - ‚úÖ Naive transpose kernel (1D thread indexing)
  - ‚úÖ Measures memory bandwidth (GB/s)
  - ‚úÖ Default sizes: 256, 512, 1024, 2048, 4096, 8192 (NxN matrices)
  - ‚úÖ Verification with float32-aware tolerance
  - Results @ 8192: Arc GPU 10.19 GB/s (11% of peak, strided writes hurt)
  
- [x] **Transpose (Tiled)** - Optimized with shared memory
  - ‚úÖ Implemented in bench_transpose_tiled.ml
  - ‚úÖ Uses 16√ó16 tiles with shared memory
  - ‚úÖ +1 padding to avoid bank conflicts
  - ‚úÖ 2D thread blocks for optimal GPU utilization
  - ‚úÖ Default sizes: 256, 512, 1024, 2048, 4096, 8192
  - Results @ 8192: Arc GPU 32.67 GB/s (3.21√ó speedup over naive!)
  - Shows excellent scaling: 0.87√ó @ 256 ‚Üí 3.21√ó @ 8192
  - CPU benefits even more: 5.37√ó speedup @ 8192
  
- [x] **Scan (Prefix Sum)** - Parallel scan algorithms
  - ‚úÖ Implemented in bench_scan.ml
  - ‚úÖ Hillis-Steele parallel scan algorithm
  - ‚úÖ Power-of-2 sizes: 64, 128, 256
  
- [x] **Gather/Scatter** - Irregular memory access
  - ‚úÖ Implemented in bench_gather_scatter.ml
  - ‚úÖ Index-based array operations (both gather and scatter)
  - ‚úÖ Measure random access performance
  - ‚úÖ Default sizes: 1M, 10M, 50M elements

### Sorting and Searching
- [x] **Bitonic Sort** - Parallel sorting network
  - ‚úÖ Implemented in bench_bitonic_sort.ml
  - ‚úÖ In-place sorting network
  - ‚úÖ Size sweep (powers of 2): 1024, 4096, 16384
  
- [x] **Radix Sort** - Integer sorting
  - ‚úÖ Implemented in bench_radix_sort.ml
  - ‚ö†Ô∏è Known issue #101: Segmentation fault
  - Multi-pass digit-based sort
  
- [x] **Histogram** - Binning with atomics
  - ‚úÖ Implemented in bench_histogram.ml
  - ‚úÖ 256 bins with atomic operations
  - ‚úÖ Default sizes: 1M, 10M, 50M elements

## Scientific Computing

### Stencil Computations
- [x] **2D Jacobi** - Iterative stencil
  - ‚úÖ Implemented in bench_stencil_2d.ml
  - ‚úÖ 5-point stencil (up, down, left, right, center)
  - ‚úÖ Heat diffusion / Laplace equation
  - ‚úÖ Default sizes: 256√ó256, 512√ó512, 1024√ó1024, 2048√ó2048
  
- [ ] **3D Stencil** - 3D heat equation
  - 7-point or 27-point stencil
  - Volume data processing
  
- [x] **Convolution 2D** - Image filtering
  - ‚úÖ Implemented in bench_conv2d.ml
  - ‚úÖ 3√ó3 box blur kernel
  - ‚úÖ Image processing workload
  - ‚úÖ Default sizes: 256√ó256, 512√ó512, 1024√ó1024, 2048√ó2048

### N-Body Simulation
- [x] **N-Body (naive)** - O(N¬≤) particle interactions
  - ‚úÖ Implemented in bench_nbody.ml
  - ‚úÖ All-pairs gravitational forces
  - ‚úÖ Particle counts: 512, 1024, 2048, 4096
  - ‚úÖ High arithmetic intensity benchmark
  
- [ ] **N-Body (optimized)** - Tiled computation
  - Shared memory optimization
  - Compare performance gains

### Monte Carlo Methods
- [ ] **Pi Estimation** - Simple Monte Carlo
  - Random point sampling
  - RNG performance measurement
  
- [ ] **Random Walk** - Stochastic simulation
  - Brownian motion simulation
  - Parallel RNG streams

## Graphics and Rendering

- [x] **Mandelbrot Set** - Embarrassingly parallel
  - ‚úÖ Implemented in bench_mandelbrot.ml
  - ‚úÖ Complex number iteration
  - ‚úÖ Generates visualization images
  - ‚úÖ Default sizes: 512√ó512, 1024√ó1024, 2048√ó2048
  
- [ ] **Ray Tracing** - Ray-sphere intersection
  - Basic ray tracing kernel
  - Already exists in tests/e2e (test_ray_ppx.ml) - adapt for benchmarking
  
- [ ] **Path Tracing** - Monte Carlo ray tracing
  - Global illumination
  - Multiple bounces

## Machine Learning Primitives

### Activation Functions
- [ ] **ReLU** - `max(0, x)`
- [ ] **Sigmoid** - `1 / (1 + exp(-x))`
- [ ] **Tanh** - Hyperbolic tangent
- [ ] **Softmax** - Normalization with exp

### Pooling Operations
- [ ] **Max Pooling** - 2x2 and 3x3 windows
- [ ] **Average Pooling** - Window averaging

### Normalization
- [ ] **Batch Normalization** - Mean/variance normalization
- [ ] **Layer Normalization** - Per-layer normalization

### Convolution (ML-style)
- [ ] **Im2Col + GEMM** - Standard CNN convolution
- [ ] **Winograd Convolution** - Fast convolution algorithm

## Microbenchmarks

### Atomic Operations
- [ ] **Atomic Add** - `atomicAdd` performance
- [ ] **Atomic CAS** - Compare-and-swap
- [ ] **Atomic Min/Max** - Comparison atomics

### Memory Hierarchy
- [ ] **Shared Memory Bank Conflicts** - Measure bank conflict impact
- [ ] **Register Spilling** - Register pressure effects
- [ ] **Cache Behavior** - L1/L2 cache utilization

### Synchronization
- [ ] **Barrier Synchronization** - Block-level barriers
  - Already exists in tests/e2e - adapt for benchmarking
- [ ] **Warp Shuffle** - Fast intra-warp communication (if available)

### Control Flow
- [ ] **Branch Divergence** - Warp divergence cost
- [ ] **Loop Unrolling** - Impact of pragma unroll

## FFT and Signal Processing

- [ ] **FFT 1D** - Fast Fourier Transform
  - Cooley-Tukey algorithm
  - Sizes: 1024, 4096, 16K, 64K points
  
- [ ] **FFT 2D** - Image frequency domain
  - 2D signal processing
  
- [ ] **Convolution (frequency domain)** - FFT-based convolution

## Advanced Features

### Multi-GPU
- [ ] **Weak Scaling** - Constant work per GPU
- [ ] **Strong Scaling** - Fixed work across GPUs
- [ ] **Peer-to-Peer Transfer** - Direct GPU-to-GPU copy

### Mixed Precision
- [ ] **FP16 Operations** - Half precision (if supported)
- [ ] **Mixed FP32/FP16** - Mixed precision training

### Compilation and Runtime
- [ ] **Kernel Compilation Time** - Measure compilation overhead
- [ ] **Kernel Launch Overhead** - Host-device sync cost
- [ ] **Memory Transfer Bandwidth** - Host ‚Üî Device transfer rates

## Implementation Status Summary

### ‚úÖ Completed (19 benchmarks)
1. ‚úÖ Vector Add - Memory bandwidth baseline
2. ‚úÖ Vector Copy - Memory transfer baseline
3. ‚úÖ STREAM Triad - Industry standard memory benchmark
4. ‚úÖ Matrix Multiplication (naive) - Basic dense linear algebra
5. ‚úÖ Matrix Multiplication (tiled) - Shared memory optimization
6. ‚úÖ Reduction (sum) - Parallel patterns
7. ‚úÖ Reduction (max) - Comparison-based reduction
8. ‚úÖ Dot Product - Combined multiply-reduce
9. ‚úÖ Transpose (naive) - Memory coalescing
10. ‚úÖ Transpose (tiled) - Optimized transpose
11. ‚úÖ Scan (prefix sum) - Hillis-Steele algorithm
12. ‚úÖ Gather/Scatter - Irregular memory access
13. ‚úÖ Bitonic Sort - Parallel sorting network
14. ‚úÖ Radix Sort - Integer sorting (‚ö†Ô∏è has segfault issue #101)
15. ‚úÖ Histogram - Atomic operations
16. ‚úÖ 2D Jacobi Stencil - Heat diffusion
17. ‚úÖ 2D Convolution - Image filtering
18. ‚úÖ N-Body - Gravitational simulation
19. ‚úÖ Mandelbrot Set - Fractal generation

### üöß In Progress / Planned
- [ ] FFT 1D/2D
- [ ] GEMM (optimized with register blocking)
- [ ] 3D Stencil
- [ ] N-Body (optimized with tiling)
- [ ] Ray Tracing
- [ ] Monte Carlo methods
- [ ] ML primitives (activation functions, pooling, normalization)
- [ ] Microbenchmarks (atomics, barriers, bank conflicts)

## Notes

- All benchmarks should include CPU baseline for verification
- Each benchmark should measure throughput (GFLOPS, GB/s, elements/sec)
- Size sweep for scalability analysis
- Multiple optimization levels where applicable (naive, tiled, optimized)
- Backend comparison (CUDA, OpenCL, Vulkan, Metal)
- Statistical analysis (mean, stddev, median, min, max)
- Self-contained JSON output for multi-machine aggregation
